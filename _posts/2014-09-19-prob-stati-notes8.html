---
layout: post
title: 概率统计学相关笔记（八）
---

<h2>{{ page.title }}</h2>
<p>这篇讲参数估计的方法。</p>
<p>假设统计模型为$$\Im =\{f(x;\theta):\theta\in\Theta\},$$其中$\Theta\subset\mathbb{R}^k$为参数空间，我们的目标是根据样本来估计参数$\theta=(\theta_1,\dots,\theta_k)$</p>
<p>通常，我们只对$\theta$的某个函数$T(\theta)$感兴趣，称之为感兴趣参数(parameter of interest)，其他的则为冗余参数(nuisance parameter)。如统计模型为所有高斯分布的集合，参数为$\mu,\sigma$，我们需要估计均值$\mu$，则$\mu$为感兴趣参数，$\sigma$则为冗余参数。</p>
<p>下面来看看几种参数估计的方法。其实无非就那么几种，矩估计法，极大似然估计法，贝叶斯估计法等。</p>
<p>先来看矩估计法。这个估计法大多是概率统计学书上所讲的第一种方法，因为其非常简单，就是使总体的j阶矩等于j阶样本矩，有多少个参数就列多少个方程，然后解出参数。其中，总体的j阶矩定义为$$\alpha_j=E_\theta(X^j)=\int x^jf(x;\theta)\,\mathrm{d}x$$j阶样本矩的定义为$$\hat{\alpha_j}=\frac{1}{n}\sum_{i=1}^nX_i^j$$</p>
<p>例子不举了，详情参考任意一本概率统计学书。矩估计一般不怎么好，但是可以作为一些迭代求解方法的初始估计值。</p>
<p>下面是大名鼎鼎的MLE，极大似然估计法。</p>
<p>现有样本$X_1,\dots,X_n$，由于他们是IID的，因此它们的联合概率分布为$$f(X_1,\dots,X_n;\theta)=\prod_{i=1}^nf(X_i;\theta)$$令$L_n(\theta)=f(X_1,\dots,X_n;\theta)$，叫做似然函数，$\theta$的极大似然估计为$$\hat{\theta_n}=arg\max_\theta L_n(\theta)$$</p>
<p>关于这个方法的解释，这里记录一段我当初上概率统计学时，老师说的话。</p>
<p>一般来说，高概率事件出现的可能肯定比低概率出现的可能大。比如某个随机变量取某个值的概率比较高，那么一次随机实验，这个值出现的可能肯定时最高的。反过来，如果一次随机试验的结果出现了一个值，那我们有理由相信这个随机变量取这个值的概率时最高的。因此，我们抽样了n个样本，既然这n个样本的结果是这样，那么我们就相信这n个样本一起出现这种结果的概率是最高的，也就是说这组样本的联合概率分布取当前样本的值的概率最大，因此，极大似然估计就是找到一个$\hat{\theta}$，使这个值最大。</p>
<p>于是，极大似然估计就是一个求解最优化问题，一般我们最优化目标函数$l(\theta)=\log L_n(\theta)$，它叫做对数似然函数，因此它和似然函数在相同的位置取极大值</p>
<p>既然是最优化问题，那么求解方法就可以有多种，比如如果似然函数或者对数似然函数的形式简单，我们可以求得解析解。但是在大多数情况下，解析解不易求得，这时就需要其他的一些最优化算法，如梯度下降，牛顿法等等。还有一种经典的求解极大似然估计的算法，如在求解高斯混合模型时非常有效的算法，叫做EM算法。</p>
<p>{{ page.date | date_to_string }}</p>

