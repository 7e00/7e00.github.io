---
layout: post
title: 概率统计学相关笔记（五）
---

<h2>{{ page.title }}</h2>
<p>
ok，好久没更了，前面的都是理论基础，地基，接下来的才是我们的房屋！好了，高潮部分正式启动，统计推断！

<strong>统计推断(statistical inference)</strong>：统计推断的问题是给定一组样本X1...Xn，去推断总体X的分布F。这个问题可庞大了，要从部分去推整体必然不是那么容易。所以有时候，我们只推断F的某些性质，比如期望。一般来说我们假设总体X的分布是在一个统计模型\(\Im\)(statistical model)里，如果这个统计模型的所有分布都能被有限个参数表示，那么这个模型就是参数模型(parametric model)，这样统计推断的问题也就简化成了参数推断的问题。注意如果我们要推断分布，那么统计模型就是分布的集合，如果我们要推断概率密度，那么统计模型就是概率密度的集合，如果我们要推断回归函数，那么统计模型就是函数的集合。<strong>总之，统计推断问题就是利用样本推断出某个和总体分布F有关的量，这个量可以是F本身，可以是F的泛函，也可以是一个回归函数等。</strong>

<strong>回归，预测和分类(regression, prediction and classification)</strong>：给定观察数据\((X_1,Y_1),\dots ,(X_n,Y_n)\)，\(r(x)=E(Y|X=x)\)称为回归函数。如果假设\(r\in\Im\)，其中\(\Im\)是有限维的，就称模型是参数回归模型，否则为非参数回归模型。给定X的值预测Y，称为预测问题(prediction)。如果Y是离散的，就称为分类(classification)。如果目标是估计函数r，就称为回归估计(regression)。每个回归模型都可以记为这种形式\[Y=r(X)+\varepsilon ,\]其中，\(E(\varepsilon)=0\)。因为\[E(\varepsilon)=E(E(\varepsilon|X))=E(E((Y-r(X))|X))=E(E(Y|X)-E(r(X)|X))=E(r(X)-r(X))=0\]

<strong>点估计(point estimation)</strong>：很多统计推断问题都可以看成是点估计，置信集和假设检验这3类问题中的一种。其中点估计是对某个感兴趣的量提供一个最优的估计，这个感兴趣的量可以是某个参数，也可以是CDF，回归函数r中的某一参数，或者可以是对某些随机变量未来值Y的预测等。假定我们要估计的某个感兴趣的量是\(\theta\)，我们有时也统称它为参数，它的点估计记作\(\hat{\theta}\)或\(\hat{\theta}_n\)。<strong>需要谨记的一点是\(\theta\)是固定的，它只和随机变量的分布有关，虽然那个分布我们不知道，但那是确定的！而\(\hat{\theta}\)是一个随机变量，它是n个样本随机变量的函数！</strong>对某个点估计\(\hat{\theta}\)，怎么评价它的好坏呢？主要有以下几个：

一致性：如果\(\hat{\theta}_n\xrightarrow{P}\theta\)，就说估计量\(\hat{\theta}_n\)是一致的。这个是评价一个估计量好坏的很重要的指标，估计量是一致的那么就说明当样本量足够多的时候，估计量非常接近真实值。

无偏性：如果\(E(\hat{\theta}_n)=\theta\)，估计量就是无偏的。记\(bias(\hat{\theta}_n)=E(\hat{\theta}_n)-\theta\)为偏差。无偏性的指标其实不怎么重要。

有效性：估计量的标准差越小，这个估计越好。估计量的标准差称为标准误差，记为se，一般情况下se和总体分布F有关，也需要估计，估计的标准误差记为\(\hat{se}\)。

均方误差：均方误差MSE定义为\[MSE=E_\theta(\hat{\theta}_n-\theta)^2.\]可以看到，MSE实际上综合考虑了无偏和有效性，如果估计量是无偏的，那么MSE实际上就是估计量的方差。事实上，有\[MSE=bias^2(\hat{\theta}_n)+V_\theta(\hat{\theta}_n).\]

渐进正态性：如果一个估计量渐进服从正态分布，即有\[\frac{\hat{\theta}_n-\theta}{se}\xrightarrow{F}N(0,1),\]一个估计量如果是渐进正态的，那么一般来说这个估计也是比较好的。事实上很多估计量都有渐进正态性，后面的最大似然估计不仅是渐进正态的，在特定前提下还是最优渐进正态的，就是说它的方差是所有估计量中的最小的。

<strong>置信集(confidence set)</strong>：也就是置信区间，只不过置信区间是相对于某一个单独的参数的估计，当参数是多维的向量的时候，自然，区间也就变成了集合。所以，只说置信区间的事了。参数\(\theta\)的1-α的置信区间\(C_n=(a,b)\)，其中，\(a=a(X_1,\dots,X_n)\)，\(b=b(X_1,\dots,X_n)\)，都是样本随机变量的函数，这个区间满足\[P_\theta(\theta\in C_n)\geq 1-\alpha\]<strong>观众朋友们请注意，这不是对\(\theta\)的概率陈述，而是对区间的概率陈述！参数\(\theta\)虽然不知道，但是固定的！而区间是随机的！这个置信区间的意思就是说我的这个随机区间(a,b)覆盖真实参数\(\theta\)的概率至少是1-α！</strong>怎么求置信区间呢？有办法的，不急。

<strong>假设检验(<span style="color: #545454;">Hypothesis Testing</span>)</strong>：记得大学初学概率统计的时候，这东西感觉和置信区间没啥区别，就换了个说法。它是数理统计学中根据一定假设条件由样本推断总体的一种方法。具体作法是：根据问题的需要对所研究的总体作某种假设，记作H0；选取合适的统计量，这个统计量的选取要使得在假设H0成立时，其分布为已知；由实测的样本，计算出统计量的值，并根据预先给定的显著性水平进行检验，作出拒绝或接受假设H0的判断(以上转自百度百科)。有点晕？举个例子吧，抛硬币。

令\(X_1,\dots,X_n\sim Bernoulli(p)\)为n次独立的抛硬币实验。假设要检验硬币是否均匀，因此我们令H0表示硬币是均匀的(原假设)，H1表示硬币不是均匀的(备择假设)，即有\[H_0:p=\frac{1}{2},\quad H_1:p\
eq \frac{1}{2}\]现在我们选一个统计量\(\bar{X}=\frac{1}{n}\sum_{i}X_i\)，由于我们假设了p的值，那就very nice了！我们就当知道了p嘛！那\(\bar{X}\)的分布我们也知道了嘛！知道了它的分布，那我就知道它的性质，比如一般它的值最可能是等于多少的，实际上它的值就大概是1/2左右嘛。这样我们做了这样n次抛硬币的独立重复实验，得到了n个值\(x_1,\dots,x_n\)，算出\(\bar{X}\)的值\(\bar{x}\)，要是这个值离1/2太远了，那我们肯定有理由拒绝原先的假设了！如果值确实在1/2左右的话，我们只能所保留原假设，并不能说原假设一定是对的，至少这次实验是没理由拒绝这个假设了。
</p>
<p>{{ page.date | date_to_string }}</p>

